{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/10_tool_RAG.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Tool RAG\n",
    "\n",
    "## Overview  \n",
    "In this exercise, we will explore the concept of Tool Retrieval-Augmented Generation (Tool-RAG) using the Solar framework. Tool-RAG integrates various tools(functions) to enhance the retrieval and generation capabilities of the model. This notebook will guide you through implementing Tool-RAG and demonstrate how it improves the accuracy and relevance of generated responses by utilizing external tools.\n",
    " \n",
    "## Purpose of the Exercise\n",
    "The purpose of this exercise is to demonstrate the integration and application of Tool-Augmented Retrieval-Augmented Generation within the Solar framework. By the end of this tutorial, users will understand how to leverage external tools to refine and augment the information retrieved and generated by the language model, leading to more precise and contextually relevant outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools in LangChain\n",
    "\n",
    "### High-Level Overview\n",
    "\n",
    "The provided code demonstrates how to create custom tools in LangChain, a framework for developing applications powered by language models. Tools are essential components in LangChain that allow language models to perform specific tasks or access external resources.\n",
    "\n",
    "The code defines three custom tools:\n",
    "\n",
    "1. `add`: A tool that adds two integers.\n",
    "2. `multiply`: A tool that multiplies two integers.\n",
    "3. `get_news`: A tool that retrieves news articles on a given topic using an external API.\n",
    "\n",
    "These tools are then bound to a language model using the `bind_tools` method, enabling the model to utilize these tools when generating responses.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "Let's break down the code and explain each part in detail:\n",
    "\n",
    "1. Importing necessary modules:\n",
    "   - `tool` from `langchain_core.tools`: This module provides the `@tool` decorator for defining custom tools.\n",
    "   - `requests`: A library for making HTTP requests to external APIs.\n",
    "\n",
    "2. Defining the `add` tool:\n",
    "   - The `@tool` decorator is used to define the `add` function as a custom tool.\n",
    "   - The function takes two integer parameters, `a` and `b`, and returns their sum.\n",
    "   - The docstring provides a brief description of the tool's functionality.\n",
    "\n",
    "3. Defining the `multiply` tool:\n",
    "   - Similar to the `add` tool, the `multiply` function is defined as a custom tool using the `@tool` decorator.\n",
    "   - It takes two integer parameters, `a` and `b`, and returns their product.\n",
    "   - The docstring describes the tool's purpose.\n",
    "\n",
    "4. Defining the `get_news` tool:\n",
    "   - The `get_news` function is defined as a custom tool using the `@tool` decorator.\n",
    "   - It takes a `topic` parameter of type `str` and returns news articles related to that topic.\n",
    "   - The function constructs a URL for the news API using the provided topic and an API key stored in an environment variable.\n",
    "   - It sends a GET request to the API using the `requests` library and returns the JSON response.\n",
    "\n",
    "5. Creating a list of tools:\n",
    "   - The `tools` list is created, containing the `add`, `multiply`, and `get_news` tools.\n",
    "   - This list will be used to bind the tools to the language model.\n",
    "\n",
    "6. Binding the tools to the language model:\n",
    "   - The `bind_tools` method of the `llm` object is called, passing the `tools` list as an argument.\n",
    "   - This step binds the custom tools to the language model, allowing it to utilize these tools when generating responses.\n",
    "   - The resulting object is assigned to the variable `llm_with_tools`.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The code demonstrates how to create custom tools in LangChain, which can be used to extend the capabilities of language models. By defining tools for specific tasks, such as mathematical operations or retrieving news articles, developers can enhance the functionality of their LangChain applications.\n",
    "\n",
    "The `@tool` decorator simplifies the process of defining custom tools, while the `bind_tools` method allows seamless integration of these tools with the language model.\n",
    "\n",
    "By leveraging custom tools, LangChain enables developers to build powerful and versatile applications that can perform a wide range of tasks beyond simple text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
    "else:\n",
    "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_summary = \"\"\"\n",
    "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\n",
    "\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def solar_paper_search(query: str) -> str:\n",
    "    \"\"\"Query for research paper about solarllm, dus, llm and general AI.\n",
    "    If the query is about DUS, Upstage, AI related topics, use this.\n",
    "    \"\"\"\n",
    "    return solar_summary\n",
    "\n",
    "\n",
    "@tool\n",
    "def internet_search(query: str) -> str:\n",
    "    \"\"\"This is for query for internet search engine like Google.\n",
    "    Query for general topics.\n",
    "    \"\"\"\n",
    "    return tavily.search(query=query)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_news(topic: str) -> str:\n",
    "    \"\"\"Get latest news about a topic.\n",
    "    If users are more like recent news, use this.\n",
    "    \"\"\"\n",
    "    # https://newsapi.org/v2/everything?q=tesla&from=2024-04-01&sortBy=publishedAt&apiKey=API_KEY\n",
    "    # change this to request news from a real API\n",
    "    news_url = f\"https://newsapi.org/v2/everything?q={topic}&apiKey={os.environ['NEWS_API_KEY']}\"\n",
    "    respnse = requests.get(news_url)\n",
    "    return respnse.json()\n",
    "\n",
    "\n",
    "tools = [solar_paper_search, internet_search, get_news]\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'solar_paper_search',\n",
       "  'args': {'query': 'Solar LLM'},\n",
       "  'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is Solar LLM?\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_news',\n",
       "  'args': {'topic': 'Seoul'},\n",
       "  'id': '9f0829a2-da28-4f39-9832-14d07df59eb0'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is top news about Seoul\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'internet_search',\n",
       "  'args': {'query': 'best place in Seoul'},\n",
       "  'id': '1f86d563-de15-460a-abc0-0e644e284518'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What's best place in Seoul?\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool_func(tool_call):\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    if tool_name not in globals():\n",
    "        print(\"Tool not found\", tool_name)\n",
    "        return None\n",
    "    selected_tool = globals()[tool_name]\n",
    "    return selected_tool.invoke(tool_call[\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide answer for question from the following context. \n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart RAG, Self-Improving RAG\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "def tool_rag(question):\n",
    "    for _ in range(3):  # try 3 times\n",
    "        tool_calls = llm_with_tools.invoke(question).tool_calls\n",
    "        if tool_calls:\n",
    "            break\n",
    "        else:\n",
    "            print(\"try again\")\n",
    "\n",
    "    if not tool_calls:\n",
    "        return \"I'm sorry, I don't have an answer for that.\"\n",
    "\n",
    "    print(tool_calls)\n",
    "    context = \"\"\n",
    "    for tool_call in tool_calls:\n",
    "        context += str(call_tool_func(tool_call))\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    return chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Solar llm is a large language model (LLM) with 10.7 billion parameters.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"What is Solar llm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed. Additionally, Tesla has announced plans to lay off more than 10% of its workforce and is facing a federal investigation into its self-driving claims. The company is also reportedly laying off hundreds more, including the majority of its Supercharging team, and has slashed prices for its vehicles in the US, China, and Europe. Tesla's CEO, Elon Musk, has also been in the news for his recent trip to China to discuss enabling autonomous driving mode on Tesla cars in the country.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"What is news about Tesla?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'internet_search', 'args': {'query': 'iPhone 13 spec'}, 'id': '7b89b621-fd4b-4bfe-9eec-dac21354d93c'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The iPhone 13 specs include a 6.1-inch display with a 2532 x 1170 pixel resolution and a 60Hz refresh rate. It is equipped with the Apple A15 Bionic chipset, 4GB of RAM, and 128GB of storage that is not expandable. The device has a 12MP dual camera and a 12MP front camera. The iPhone 13 has a ceramic shield glass and a weight of 6.14 ounces. It has a built-in stereo speaker and microphone, as well as a Lightning connector. The battery life is up to 19 hours for video playback and up to 75 hours for audio playback. The device also supports various languages and has built-in accessibility features.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"iPhone 13 spec?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "Solar LLM is small, so it might not give the best results for complex tasks. For those, you can use larger LLMs like GPT-4. However, for quick answers and summaries, using a small size LLM like Solar can give better performance and efficiency.\n",
    "\n",
    "Please note that good engineering involves making things work with limited components.\n",
    "![Engineering](figures/engineering.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
