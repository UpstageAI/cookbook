{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOMFMnjx9K8E"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/12_chat_with_history.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2hTTQk99K8F"
      },
      "source": [
        "# 12. Chat with History\n",
        "\n",
        "## Overview  \n",
        "This exercise demonstrates the implementation of a question-answering system that utilizes user conversation history. It constructs a chain to generate appropriate responses to user questions based on previous conversation records. Additionally, it allows for more precise question-answering tasks by providing specific context.\n",
        "\n",
        "## Purpose of the Exercise\n",
        "The purpose of this exercise is to enhance the ability to manage questions and responses using conversation history. By the end of this tutorial, users will understand how to create a system that uses past interactions to inform current responses, thereby improving the accuracy and relevance of the answers provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6jtzuGF9K8F"
      },
      "outputs": [],
      "source": [
        "! pip3 install -qU langchain-upstage  python-dotenv python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NddEAxT9K8G"
      },
      "outputs": [],
      "source": [
        "# @title set API key\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
        "else:\n",
        "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "assert (\n",
        "    \"UPSTAGE_API_KEY\" in os.environ\n",
        "), \"Please set the UPSTAGE_API_KEY environment variable\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fg3kyTpa9K8G"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import ChatUpstage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an assistant for question-answering tasks. \"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = qa_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YeiogLaA9K8G",
        "outputId": "89859017-fff7-445a-d843-b4d7dc553944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could you please provide more context or specify what information you need about Korea? Are you interested in its geography, history, culture, or something else?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "question = \"How about Korea?\"\n",
        "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": []})\n",
        "print(ai_msg_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A-_-K44p9K8H",
        "outputId": "497980d0-433b-4697-eb38-2e232b2b192d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n",
            "The capital of South Korea is Seoul, while the capital of North Korea is Pyongyang.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "question = \"Where is the capital of France?\"\n",
        "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "print(ai_msg_1)\n",
        "chat_history.extend([HumanMessage(question), AIMessage(ai_msg_1)])\n",
        "\n",
        "\n",
        "second_question = \"How about Korea?\"\n",
        "ai_msg_2 = chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "chat_history.extend([HumanMessage(second_question), AIMessage(ai_msg_2)])\n",
        "\n",
        "print(ai_msg_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rfqcDREO9K8H"
      },
      "outputs": [],
      "source": [
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = qa_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MyfvjpIA9K8I"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters,\n",
        "    demonstrating superior performance in various natural language processing (NLP) tasks.\n",
        "    Inspired by recent efforts to efficiently up-scale LLMs,\n",
        "    we present a method for scaling LLMs called depth up-scaling (DUS),\n",
        "    which encompasses depthwise scaling and continued pretraining.\n",
        "    In contrast to other LLM up-scaling methods that use mixture-of-experts,\n",
        "    DUS does not require complex changes to train and inference efficiently.\n",
        "    We show experimentally that DUS is simple yet effective\n",
        "    in scaling up high-performance LLMs from small ones.\n",
        "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\n",
        "    a variant fine-tuned for instruction-following capabilities,\n",
        "    surpassing Mixtral-8x7B-Instruct.\n",
        "    SOLAR 10.7B is publicly available under the Apache 2.0 license,\n",
        "    promoting broad access and application in the LLM field.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LUQ3hJL49K8I",
        "outputId": "64e632dc-5e33-4489-b153-4c0a0c788ff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1 DUS stands for Depth Up-Scaling, a method for scaling large language models (LLMs) introduced in the context of SOLAR 10.7B. It involves depthwise scaling and continued pretraining, and is simpler yet effective compared to other LLM up-scaling methods that use mixture-of-experts.\n",
            "A2 The benefit of DUS is that it allows for efficient scaling up of high-performance LLMs from smaller ones without requiring complex changes for efficient training and inference, as some other methods do.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "question = \"What is DUS?\"\n",
        "ai_msg_1 = chain.invoke(\n",
        "    {\"input\": question, \"chat_history\": chat_history, \"context\": context}\n",
        ")\n",
        "chat_history += [HumanMessage(question), AIMessage(ai_msg_1)]\n",
        "print(\"A1\", ai_msg_1)\n",
        "\n",
        "second_question = \"What's the benefit?\"\n",
        "ai_msg_2 = chain.invoke(\n",
        "    {\"input\": second_question, \"chat_history\": chat_history, \"context\": context}\n",
        ")\n",
        "chat_history += [HumanMessage(second_question), AIMessage(ai_msg_2)]\n",
        "\n",
        "print(\"A2\", ai_msg_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sSbr5fK99K8I",
        "outputId": "3a3d81ce-cc50-4569-df22-0836729c5d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='What is DUS?' additional_kwargs={} response_metadata={}\n",
            "content='DUS stands for Depth Up-Scaling, a method for scaling large language models (LLMs) introduced in the context of SOLAR 10.7B. It involves depthwise scaling and continued pretraining, and is simpler yet effective compared to other LLM up-scaling methods that use mixture-of-experts.' additional_kwargs={} response_metadata={}\n",
            "content=\"What's the benefit?\" additional_kwargs={} response_metadata={}\n",
            "content='The benefit of DUS is that it allows for efficient scaling up of high-performance LLMs from smaller ones without requiring complex changes for efficient training and inference, as some other methods do.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "for chat in chat_history:\n",
        "    print(chat)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}