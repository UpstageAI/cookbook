{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/05_2_MongoDB.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-2. SemanticSearch_MongoDB\n",
    "\n",
    "\n",
    "## Overview  \n",
    "In this exercise, we will explore how to utilize MongoDB to embed documents and construct a vectorspace. You will learn how to create a Retriever object and conduct hybrid searches to achieve effective query results. Additionally, this exercise covers performing additional keyword searches utilizing the Atlas Index available in MongoDB, enhancing the search capabilities.\n",
    " \n",
    "## Purpose of the Exercise\n",
    "The purpose of this exercise is to demonstrate the use of the Solar Embedding API to generate embeddings and create a vectorspace. By the end of this tutorial, users will understand how to conduct query searches using the Hybrid Search method, deploy a cluster to leverage MongoDB Atlas, and perform additional keyword searches based on the MongoDB Atlas Index. This exercise will enhance your ability to perform comprehensive and effective searches within MongoDB, utilizing both embeddings and keyword search techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Atlas\n",
    "\n",
    "To use MongoDB Atlas, you must first deploy a cluster. To get started head over to Atlas here: [quick start](https://www.mongodb.com/docs/atlas/getting-started/).\n",
    "Create an Atlas database and create an Atlas Search Index to search vectors.\n",
    "\n",
    "Follow below MongoDB Atlas guide\n",
    "- [Create new cluster](https://www.mongodb.com/docs/atlas/tutorial/create-new-cluster/)\n",
    "- [Connect to database](https://www.mongodb.com/docs/atlas/driver-connection/)\n",
    "- [Create an Atlas Vector Search Index](https://www.mongodb.com/docs/atlas/atlas-vector-search/create-index/)\n",
    "- [Create Index Fields](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#std-label-avs-types-vector-search)\n",
    "\n",
    "### benefits?\n",
    "- use mongoDB itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25 pymongo langchain langchain-mongodb python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "UPSTAGE_API_KEY = getpass.getpass(\"Enter your API Key\")\n",
    "_ = os.environ.setdefault(\"UPSTAGE_API_KEY\", UPSTAGE_API_KEY)\n",
    "\n",
    "MONGODB_ATLAS_CLUSTER_URI = getpass.getpass(\"Enter your MONGODB ATLAS CLUSTER URI\")\n",
    "_ = os.environ.setdefault(\"MONGODB_ATLAS_CLUSTER_URI\", MONGODB_ATLAS_CLUSTER_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
    "else:\n",
    "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
]

  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Your connection string should use following format:\n",
    "mongodb+srv://<username>:<password>@<clusterName>.<hostname>.mongodb.net\n",
    "\"\"\"\n",
    "MONGODB_ATLAS_CLUSTER_URI = os.environ[\"MONGODB_ATLAS_CLUSTER_URI\"]\n",
    "\n",
    "# Connect to your Atlas cluster\n",
    "client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n",
    "# Define collection and index name\n",
    "DB_NAME = \"langchain_db\"\n",
    "COLLECTION_NAME = \"test\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "db_collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexes\n",
    "\n",
    "create atlas index fields.\n",
    "\n",
    "[Create Index Fields](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#std-label-avs-types-vector-search)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"numDimensions\": 4096,\n",
    "      \"path\": \"embedding\",\n",
    "      \"similarity\": \"dotProduct\",\n",
    "      \"type\": \"vector\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Korea is a beautiful country to visit in the spring.'), Document(page_content='The best time to visit Korea is in the fall.'), Document(page_content='Best way to find bug is using unit test.'), Document(page_content='Python is a great programming language for beginners.'), Document(page_content='Sung Kim is a great teacher.')]\n",
      "batch_size: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "sample_text = [\n",
    "    \"Korea is a beautiful country to visit in the spring.\",\n",
    "    \"The best time to visit Korea is in the fall.\",\n",
    "    \"Best way to find bug is using unit test.\",\n",
    "    \"Python is a great programming language for beginners.\",\n",
    "    \"Sung Kim is a great teacher.\",\n",
    "]\n",
    "\n",
    "splits = RecursiveCharacterTextSplitter().create_documents(sample_text)\n",
    "\n",
    "print(splits)\n",
    "\n",
    "vectorstore = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=splits,\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection.find_one({\"text\": \"Hello, new sentence\"}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection.find_one({\"text\": splits[0].page_content}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 125\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "vectorstore = MongoDBAtlasVectorSearch(\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "unique_splits = [\n",
    "    split\n",
    "    for split in splits\n",
    "    if not db_collection.find_one({\"text\": split.page_content})\n",
    "]\n",
    "print(len(unique_splits))\n",
    "\n",
    "# 3. Embed & indexing if it's not in the vector store\n",
    "if len(unique_splits) > 0:\n",
    "    MongoDBAtlasVectorSearch.from_documents(\n",
    "        documents=unique_splits,\n",
    "        collection=MONGODB_COLLECTION,\n",
    "        embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retriever\n",
    "search_result = retriever.invoke(\"How to find problems in code?\")\n",
    "print(search_result)\n",
    "print(search_result[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hybrid search\n",
    "\n",
    "We will carry out a hybrid search that combines BM25 and vector search techniques. This process will unfold in two separate stages: first within LangChain, and then via a query in MongoDB. Moreover, we will apply reciprocal rank fusion to merge results from different search methods into a single, unified outcome.\n",
    "\n",
    "Add additional **Search Index** to MongoDB Atlas with following definition. To enable keyword search.\n",
    "```json\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"dynamic\": false,\n",
    "    \"fields\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "# Using Langchain\n",
    "\n",
    "\n",
    "# 1. initializing retrievers\n",
    "vectorstore = MongoDBAtlasVectorSearch(\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    ")\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "\n",
    "# set weights for reciprocal rank fusion\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever], weights=[0.4, 0.6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"<p id='13' style='font-size:16px'>introduced bugs immediately. Several bug-finding techni-<br>ques could be used, including code inspections, unit testing,<br>and the use of static analysis tools. Since these steps would<br>be taken right after a code change was made, the developer<br>would still retain the full mental context of the change. This<br>holds promise for reducing the time required to find<br>software bugs and reducing the time that bugs stay resident<br>in software before removal.</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f7d'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='42' style='font-size:16px'>determine which kinds of function return values must be<br>checked. For example, if the return value of foo was always<br>verified in the previous project history but was not verified<br>in the current source code, it is very suspicious. Livshits and<br>Zimmermann combine software repository mining and<br>dynamic analysis to discover common use patterns and<br>code patterns that are likely errors in Java applications [25].<br>Similarly, PR-Miner mines common call sequences from a<br>code snapshot and then marks all noncommon call patterns<br>as potential bugs [24].</p><br><p id='43' style='font-size:16px'>These approaches are similar to change classification<br>since they use project specific patterns to determine latent<br>software bugs. However, the mining is limited to specific<br>patterns such as return types or call sequences and hence<br>limits the type of latent bugs that can be identified.</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f91'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='39' style='font-size:20px'>2.2 Mining Buggy Patterns</p><br><p id='40' style='font-size:16px'>One thread of research attempts to find buggy or clean code<br>patterns in the history of development of a software project.</p><br><p id='41' style='font-size:16px'>Williams and Hollingsworth use project histories to<br>improve existing bug-finding tools [51]. Using a return<br>value without first checking its validity may be a latent bug.<br>In practice, this approach leads to many false positives as<br>typical code has many locations where return values are<br>used without checking. To remove the false positives,<br>Williams and Hollingsworth use project histories to</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f90'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content='<br>the latent bugs. In contrast, efforts that detect faults by<br>analyzing the source code by using static or dynamic<br>analysis techniques can identify specific kinds of bugs in the<br>software, though generally with high rates of false positives.<br>Common techniques include type checking, deadlock<br>detection, and pattern recognition [8], [26], [48].</p><br>', metadata={'_id': {'$oid': '66500f045d6a17e9c9316f88'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='37' style='font-size:16px'>Khoshgoftaar and Allen have proposed a model to list<br>modules according to software quality factors such as<br>future fault density [15], [16], [21]. The inputs to the model<br>are software complexity metrics such as LOC, number of<br>unique operators, and cyclomatic complexity. A stepwise<br>multiregression is then performed to find weights for each<br>factor [15], [16]. Mockus and Weiss predict risky modules in<br>software by using a regression algorithm and change<br>measures such as the number of systems touched, the<br>number of modules touched, the number of lines of added<br>code, and the number of modification requests [33].</p><br><p id='38' style='font-size:16px'>Pan et al. use metrics computed over software slice data<br>in conjunction with machine learning algorithms to find<br>bug-prone software files or functions [41]. Their approach<br>tries to find faults in the whole code, while our approach<br>focuses on file changes.</p><br>\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='48' style='font-size:16px'>Similar in spirit to change classification is work that<br>classifies bug reports or software maintenance requests [3],<br>[10]. In this research, keywords in bug reports or change<br>requests are extracted and used as features to train a<br>machine learning classifier. The goal of the classification is<br>to place a bug report into a specific category or to find the<br>developer best suited to fix a bug. This work, along with<br>change classification, highlights the potential of using<br>machine learning techniques in software engineering. If</p><p id='51' style='font-size:16px'>an existing concern such as assigning bugs to developers<br>can be recast as a classification problem, then it is possible<br>to leverage the large collection of data stored in bug<br>tracking and SCM systems.</p><br><p id='52' style='font-size:20px'>2.4 Text Classification</p><br>\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='47' style='font-size:16px'>Research that categorizes or associates source code with<br>other documents (traceability recovery) is similar to ours in<br>that it gathers terms from source code and then uses<br>learning or statistical approaches to find associated docu-<br>ments [2], [42]. For example, Maletic et al. [28], [29]<br>extracted all features available in the source code via Latent<br>Semantic Analysis (LSA) and then used this data to cluster<br>software and to create relationships between the source<br>code and other related software project documents. In a<br>similar vein, Kuhn et al. use partial terms from the source<br>code to cluster the code to detect abnormal module<br>structures [20]. Antoniol et al. use stochastic modeling<br>and Bayesian classification for traceability recovery [2].<br>Their work differs from ours in that they only use features<br>from the source code, while our change classification learns\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content='<br>the prediction granularity of change classification is much<br>finer, file level changes , which, for the projects that we<br>analyzed, average 20 lines of code (LOCs) per change. This<br>is significant since developers need to examine an order of<br>magnitude smaller number of LOCs to find latent bugs with<br>the change classification approach. Gyimothy et al. use<br>release-based classes for prediction, where a release is an<br>accumulation of many versions, while our change classifi-<br>cation applies to the changes between successive individual<br>versions. This allows change classification to be used in an<br>ongoing daily manner instead of just for releases which<br>occur on months-long time scales.</p><br>', metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'})]\n"
     ]
    }
   ],
   "source": [
    "# 2. Query using hybrid retriever\n",
    "docs = hybrid_retriever.get_relevant_documents(\"How to find prblems in code?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MongoDB query\n",
    "\n",
    "# One of the great things about the MongoDB Atlas vector store is the variety of queries we can use.\n",
    "# Perfrom hybrid search using MongoDB query.\n",
    "\n",
    "# The reciprocal rank score is calculated as below\n",
    "# 1.0/{document position in the results + vector or full-text penalty + constant value}\n",
    "\n",
    "\n",
    "def hybrid_search(client, query):\n",
    "    vector_penalty = 4\n",
    "    keyword_penalty = 6\n",
    "    return client.aggregate(\n",
    "        [\n",
    "            {\n",
    "                # $vectorSearch stage to search the embedding field for the query specified as vector embeddings in the queryVector field of the query.\n",
    "                # The query specifies a search for up to 100 nearest neighbors and limit the results to 20 documents only. This stage returns the sorted documents from the semantic search in the results.\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": UpstageEmbeddings(\n",
    "                        model=\"solar-embedding-1-large\"\n",
    "                    ).embed_query(query),\n",
    "                    \"numCandidates\": 10,\n",
    "                    \"limit\": 5,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $group stage to group all the documents in the results from the semantic search in a field named docs.\n",
    "                \"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}\n",
    "            },\n",
    "            {\n",
    "                # $unwind stage to unwind the array of documents in the docs field and store the position of the document in the results array in a field named rank.\n",
    "                \"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}\n",
    "            },\n",
    "            {\n",
    "                # $addFields stage to add a new field named vs_score that contains the reciprocal rank score for each document in the results.\n",
    "                # Here, reciprocal rank score is calculated by dividing 1.0 by the sum of rank, the vector_penalty weight, and a constant value of 1.\n",
    "                \"$addFields\": {\n",
    "                    \"vs_score\": {\n",
    "                        \"$divide\": [1.0, {\"$add\": [\"$rank\", vector_penalty, 1]}]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to include only the following fields in the results: vs_score, _id, title, text\n",
    "                \"$project\": {\n",
    "                    \"vs_score\": 1,\n",
    "                    \"_id\": \"$docs._id\",\n",
    "                    \"title\": \"$docs.title\",\n",
    "                    \"text\": \"$docs.text\",\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $unionWith stage to combine the results from the preceding stages with the results of the following stages in the sub-pipeline\n",
    "                \"$unionWith\": {\n",
    "                    \"coll\": COLLECTION_NAME,\n",
    "                    \"pipeline\": [\n",
    "                        {\n",
    "                            # $search stage to search for movies that contain the query in the text field. This stage returns the sorted documents from the keyword search in the results.\n",
    "                            \"$search\": {\n",
    "                                \"index\": \"text\",\n",
    "                                \"phrase\": {\"query\": query, \"path\": \"text\"},\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            # $limit stage to limit the output to 15 results only.\n",
    "                            \"$limit\": 15\n",
    "                        },\n",
    "                        {\n",
    "                            # $group stage to group all the documents from the keyword search in a field named docs.\n",
    "                            \"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}\n",
    "                        },\n",
    "                        {\n",
    "                            # $unwind stage to unwind the array of documents in the docs field and store the position of the document in the results array in a field named rank.\n",
    "                            \"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}\n",
    "                        },\n",
    "                        {\n",
    "                            # $addFields stage to add a new field named kws_score that contains the reciprocal rank score for each document in the results.\n",
    "                            # Here, reciprocal rank score is calculated by dividing 1.0 by the sum of the value of rank, the full_text penalty weight, and a constant value of 1.\n",
    "                            \"$addFields\": {\n",
    "                                \"kws_score\": {\n",
    "                                    \"$divide\": [\n",
    "                                        1.0,\n",
    "                                        {\"$add\": [\"$rank\", keyword_penalty, 1]},\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            # $project stage to include only the following fields in the results: kws_score, _id, title, text\n",
    "                            \"$project\": {\n",
    "                                \"kws_score\": 1,\n",
    "                                \"_id\": \"$docs._id\",\n",
    "                                \"title\": \"$docs.title\",\n",
    "                                \"text\": \"$docs.text\",\n",
    "                            }\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to include only the following fields in the results: _id, title, text, vs_score, kws_score\n",
    "                \"$project\": {\n",
    "                    \"title\": 1,\n",
    "                    \"vs_score\": {\"$ifNull\": [\"$vs_score\", 0]},\n",
    "                    \"kws_score\": {\"$ifNull\": [\"$kws_score\", 0]},\n",
    "                    \"text\": 1,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to add a field named score that contains the sum of vs_score and kws_score to the results.\n",
    "                \"$project\": {\n",
    "                    \"score\": {\"$add\": [\"$kws_score\", \"$vs_score\"]},\n",
    "                    \"title\": 1,\n",
    "                    \"vs_score\": 1,\n",
    "                    \"kws_score\": 1,\n",
    "                    \"text\": 1,\n",
    "                }\n",
    "            },\n",
    "            # $sort stage to sort the results by score in descending order.\n",
    "            {\"$sort\": {\"score\": -1}},\n",
    "            #   $limit stage to limit the output to 10 results only.\n",
    "            {\"$limit\": 10},\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id='13' style='font-size:16px'>introduced bugs immediately. Several bug-finding techni-<br>ques could be used, including code inspections, unit testing,<br>and the use of static analysis tools. Since these steps would<br>be taken right after a code change was made, the developer<br>would still retain the full mental context of the change. This<br>holds promise for reducing the time required to find<br>software bugs and reducing the time that bugs stay resident<br>in software before removal.</p><br> 0.2 \n",
      "\n",
      "<p id='42' style='font-size:16px'>determine which kinds of function return values must be<br>checked. For example, if the return value of foo was always<br>verified in the previous project history but was not verified<br>in the current source code, it is very suspicious. Livshits and<br>Zimmermann combine software repository mining and<br>dynamic analysis to discover common use patterns and<br>code patterns that are likely errors in Java applications [25].<br>Similarly, PR-Miner mines common call sequences from a<br>code snapshot and then marks all noncommon call patterns<br>as potential bugs [24].</p><br><p id='43' style='font-size:16px'>These approaches are similar to change classification<br>since they use project specific patterns to determine latent<br>software bugs. However, the mining is limited to specific<br>patterns such as return types or call sequences and hence<br>limits the type of latent bugs that can be identified.</p><br> 0.16666666666666666 \n",
      "\n",
      "<p id='39' style='font-size:20px'>2.2 Mining Buggy Patterns</p><br><p id='40' style='font-size:16px'>One thread of research attempts to find buggy or clean code<br>patterns in the history of development of a software project.</p><br><p id='41' style='font-size:16px'>Williams and Hollingsworth use project histories to<br>improve existing bug-finding tools [51]. Using a return<br>value without first checking its validity may be a latent bug.<br>In practice, this approach leads to many false positives as<br>typical code has many locations where return values are<br>used without checking. To remove the false positives,<br>Williams and Hollingsworth use project histories to</p><br> 0.14285714285714285 \n",
      "\n",
      "<br>the latent bugs. In contrast, efforts that detect faults by<br>analyzing the source code by using static or dynamic<br>analysis techniques can identify specific kinds of bugs in the<br>software, though generally with high rates of false positives.<br>Common techniques include type checking, deadlock<br>detection, and pattern recognition [8], [26], [48].</p><br> 0.125 \n",
      "\n",
      "<p id='35' style='font-size:16px'>Brun and Ernst [5] use two classification algorithms to<br>find hidden code errors. Using Ernst’s Daikon dynamic<br>invariant detector, invariant features are extracted from<br>code with known errors and with errors removed. They<br>train SVM and decision tree classifiers by using the<br>extracted features and then classify invariants in the source<br>code as either fault invariant or non-fault-invariant. The<br>fault-invariant information is used to find hidden errors in<br>the source code. The reported classification accuracy is<br>10.6 percent on average (9 percent for C and 12.2 percent for<br>Java), with a classification precision of 21.6 percent on<br>average (10 percent for C and 33.2 percent for Java), and the<br>best classification precision (with the top 80 relevant<br>invariants) of 52 percent on average (45 percent for C and<br>59 percent for Java). The classified fault invariants guide 0.1111111111111111 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = hybrid_search(db_collection, \"How to find prblems in code?\")\n",
    "for doc in result:\n",
    "    print(doc[\"text\"], doc[\"score\"], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
