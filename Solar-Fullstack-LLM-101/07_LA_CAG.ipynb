{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/07_LA_CAG.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. LA CAG (Credibility-Aware Generation)\n",
    "\n",
    "## Overview  \n",
    "In this exercise, we will explore Language Analysis (LA) combined with Credibility-Aware Generation (CAG) using the Solar framework. This notebook will demonstrate how to analyze language data for credibility and generate reliable outputs. The techniques covered will enhance the accuracy and trustworthiness of text generated from various language inputs.\n",
    " \n",
    "## Purpose of the Exercise\n",
    "The purpose of this exercise is to integrate Language Analysis with Credibility-Aware Generation to produce credible and well-analyzed outputs. By the end of this tutorial, users will be able to analyze text for credibility and apply these insights to generate reliable and accurate responses using the Solar framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No.1 accuracy in multiform table extraction \n",
    "- Convert documents to maximize RAG performance \n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "\n",
    "\n",
    "![Layout Analyzer](./figures/la.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage==0.1.8rc0  requests  python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
    "else:\n",
    "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
]

  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layout Analyzer](./figures/solar_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/solar_sample.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='0' style='font-size:14px'><tr><td>Model</td><td>Size</td><td>Type</td><td>H6 (Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>SOLAR 10.7B-Instruct</td><td>11B ⇠</td><td>Alignment-tuned</td><td>74.20</td><td>71.08</td><td>88.16</td><td>66.21</td><td>71.43</td><td>83.58</td><td>64.75</td></tr><tr><td>Qwen 72B</td><td>72B ⇠</td><td>Pretrained</td><td>73.60</td><td>65.19</td><td>85.94</td><td>77.37</td><td>60.19</td><td>82.48</td><td>70.43</td></tr><tr><td>Mixtral 8x7B-Instruct-v0.1</td><td>47B ⇠</td><td>Instruction-tuned</td><td>72.62</td><td>70.22</td><td>87.63</td><td>71.16</td><td>64.58</td><td>81.37</td><td>60.73</td></tr><tr><td>Yi 34B-200K</td><td>34B ⇠</td><td>Pretrained</td><td>70.81</td><td>65.36</td><td>85.58</td><td>76.06</td><td>53.64</td><td>82.56</td><td>61.64</td></tr><tr><td>Yi 34B</td><td>34B ⇠</td><td>Pretrained</td><td>69.42</td><td>64.59</td><td>85.69</td><td>76.35</td><td>56.23</td><td>83.03</td><td>50.64</td></tr><tr><td>Mixtral 8x7B-v0.1</td><td>47B ⇠</td><td>Pretrained</td><td>68.42</td><td>66.04</td><td>86.49</td><td>71.82</td><td>46.78</td><td>81.93</td><td>57.47</td></tr><tr><td>Llama 2 70B</td><td>70B ⇠</td><td>Pretrained</td><td>67.87</td><td>67.32</td><td>87.33</td><td>69.83</td><td>44.92</td><td>83.74</td><td>54.06</td></tr><tr><td>Falcon 180B</td><td>180B ⇠</td><td>Pretrained</td><td>67.85</td><td>69.45</td><td>88.86</td><td>70.50</td><td>45.47</td><td>86.90</td><td>45.94</td></tr><tr><td>SOLAR 10.7B</td><td>11B ⇠</td><td>Pretrained</td><td>66.04</td><td>61.95</td><td>84.60</td><td>65.48</td><td>45.04</td><td>83.66</td><td>55.50</td></tr><tr><td>Qwen 14B</td><td>14B ⇠</td><td>Pretrained</td><td>65.86</td><td>58.28</td><td>83.99</td><td>67.70</td><td>49.43</td><td>76.80</td><td>58.98</td></tr><tr><td>Mistral 7B-Instruct-v0.2</td><td>7B ⇠</td><td>Instruction-tuned</td><td>65.71</td><td>63.14</td><td>84.88</td><td>60.78</td><td>68.26</td><td>77.19</td><td>40.03</td></tr><tr><td>Yi 34B-Chat</td><td>34B ⇠</td><td>Instruction-tuned</td><td>65.32</td><td>65.44</td><td>84.16</td><td>74.90</td><td>55.37</td><td>80.11</td><td>31.92</td></tr><tr><td>Mistral 7B</td><td>7B ⇠</td><td>Pretrained</td><td>60.97</td><td>59.98</td><td>83.31</td><td>64.16</td><td>42.15</td><td>78.37</td><td>37.83</td></tr></table><p id='1' data-category='paragraph' style='font-size:16px'>Table 2: Evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with<br>other top-performing models. We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score<br>(average of six tasks). We also report the size of the models in units of billions of parameters. The type indicates the<br>training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on<br>SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold.</p><h1 id='2' style='font-size:20px'>MetaMathQA (Yu et al., 2023) dataset.</h1><br><p id='3' data-category='paragraph' style='font-size:20px'>We reformatted the instruction datasets with an<br>Alpaca-styled chat template. For datasets such as<br>OpenOrca, which are derived from FLAN (Long-<br>pre et al., 2023), we ﬁlter data that overlaps with<br>the benchmark datasets (see Tab. 8 in Appendix. C<br>for more information). The alignment datasets<br>are in the {prompt, chosen, rejected} triplet for-<br>mat. We preprocess the alignment datasets follow-<br>ing Zephyr (Tunstall et al., 2023). We use Data-<br>verse (Park et al., 2024) for data preprocessing.</p><br><p id='4' data-category='paragraph' style='font-size:20px'>Evaluation. In the HuggingFace Open LLM<br>Leaderboard (Beeching et al., 2023), six types of<br>evaluation methods are presented: ARC (Clark<br>et al., 2018), HellaSWAG (Zellers et al., 2019),<br>MMLU (Hendrycks et al., 2020), TruthfulQA (Lin<br>et al., 2022), Winogrande (Sakaguchi et al., 2021),<br>and GSM8K (Cobbe et al., 2021). We utilize these<br>datasets as benchmarks for evaluation and also re-<br>port the average scores for the six tasks, e.g., H6.<br>We either submit directly to the Open LLM Leader-<br>board or utilize Evalverse (Kim et al., 2024b) for<br>running evaluations locally.</p><br><p id='5' data-category='paragraph' style='font-size:20px'>Model merging. Model merging methods such<br>as Yadav et al. (2023) can boost model perfor-<br>mance without further training. We merge some<br>of the models that we trained in both the instruc-<br>tion and alignment tuning stages. We implement<br>our own merging methods although popular open<br>source also exist such as MergeKit3.</p><br><h1 id='6' style='font-size:20px'>4.2 Main Results</h1><br><p id='7' data-category='paragraph' style='font-size:20px'>We present evaluation results for our SOLAR<br>10.7B and SOLAR 10.7B-Instruct models along</p><br><p id='8' data-category='paragraph' style='font-size:20px'>with other top-performin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    Think step by step and look the html tags and table values carefully to provide the most correct answer.\n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 2 presents evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with other top-performing models. The table includes scores for six tasks mentioned in Sec. 4.1, along with the H6 score (average of six tasks). The table also reports the size of the models in units of billions of parameters and indicates the training stage of the model (Pretrained, Instruction-tuned, or Alignment-tuned). Models based on SOLAR 10.7B are colored purple, and the best scores for H6 and the individual tasks are shown in bold.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Explain Table 2?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The MMLU score of SOLAR 10.7B is 65.48.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is MMLU scores of SOLAR 10.7B?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The MMLU scores of Mistral 7B-Instruct-v0.2 is 60.78.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"question\": \"What is MMLU scores of Mistral 7B-Instruct-v0.2?\", \"Context\": docs}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise \n",
    "Sometimes, even if we provide a table in Markdown or HTML format, the Large Language Model (LLM) may not extract the information correctly. How can you fix this issue?\n",
    "\n",
    "Hint: Consider using CoT, a few-shot learning approach or a divide and conquer strategy. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
