{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rMwqdtbvj_f"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/07_LA_CAG.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78U67wdBvj_h"
      },
      "source": [
        "# 07. LA CAG (Credibility-Aware Generation)\n",
        "\n",
        "## Overview  \n",
        "In this exercise, we will explore Language Analysis (LA) combined with Credibility-Aware Generation (CAG) using the Solar framework. This notebook will demonstrate how to analyze language data for credibility and generate reliable outputs. The techniques covered will enhance the accuracy and trustworthiness of text generated from various language inputs.\n",
        "\n",
        "## Purpose of the Exercise\n",
        "The purpose of this exercise is to integrate Language Analysis with Credibility-Aware Generation to produce credible and well-analyzed outputs. By the end of this tutorial, users will be able to analyze text for credibility and apply these insights to generate reliable and accurate responses using the Solar framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xDBnstZvj_i"
      },
      "source": [
        "# No.1 accuracy in multiform table extraction\n",
        "- Convert documents to maximize RAG performance\n",
        "- LangChain provides powerful tools for text splitting and vectorization\n",
        "\n",
        "\n",
        "![Layout Analyzer](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/la.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XthnrMXAvj_i"
      },
      "outputs": [],
      "source": [
        "! pip3 install -qU  markdownify  langchain-upstage  requests  python-dotenv langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WL_FEteevj_j"
      },
      "outputs": [],
      "source": [
        "# @title set API key\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
        "else:\n",
        "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "assert (\n",
        "    \"UPSTAGE_API_KEY\" in os.environ\n",
        "), \"Please set the UPSTAGE_API_KEY environment variable\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yel4rBSvvj_j"
      },
      "source": [
        "![Layout Analyzer](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/solar_sample.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9kNhA3Qkvj_k"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import UpstageDocumentParseLoader\n",
        "\n",
        "\n",
        "layzer = UpstageDocumentParseLoader(\"pdfs/solar_sample.pdf\", output_format=\"html\")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "docs = layzer.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "CNVmRpV9vj_k",
        "outputId": "6f4f3a6c-e17a-4413-f7c9-3c51032d8d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table id='0' style='font-size:14px'><tr><td>Model</td><td>Size</td><td>Type</td><td>H6 (Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>SOLAR 10.7B-Instruct</td><td>11B ⇠</td><td>Alignment-tuned</td><td>74.20</td><td>71.08</td><td>88.16</td><td>66.21</td><td>71.43</td><td>83.58</td><td>64.75</td></tr><tr><td>Qwen 72B</td><td>72B ⇠</td><td>Pretrained</td><td>73.60</td><td>65.19</td><td>85.94</td><td>77.37</td><td>60.19</td><td>82.48</td><td>70.43</td></tr><tr><td>Mixtral 8x7B-Instruct-v0.1</td><td>47B ⇠</td><td>Instruction-tuned</td><td>72.62</td><td>70.22</td><td>87.63</td><td>71.16</td><td>64.58</td><td>81.37</td><td>60.73</td></tr><tr><td>Yi 34B-200K</td><td>34B ⇠</td><td>Pretrained</td><td>70.81</td><td>65.36</td><td>85.58</td><td>76.06</td><td>53.64</td><td>82.56</td><td>61.64</td></tr><tr><td>Yi 34B</td><td>34B ⇠</td><td>Pretrained</td><td>69.42</td><td>64.59</td><td>85.69</td><td>76.35</td><td>56.23</td><td>83.03</td><td>50.64</td></tr><tr><td>Mixtral 8x7B-v0.1</td><td>47B ⇠</td><td>Pretrained</td><td>68.42</td><td>66.04</td><td>86.49</td><td>71.82</td><td>46.78</td><td>81.93</td><td>57.47</td></tr><tr><td>Llama 2 70B</td><td>70B ⇠</td><td>Pretrained</td><td>67.87</td><td>67.32</td><td>87.33</td><td>69.83</td><td>44.92</td><td>83.74</td><td>54.06</td></tr><tr><td>Falcon 180B</td><td>180B ⇠</td><td>Pretrained</td><td>67.85</td><td>69.45</td><td>88.86</td><td>70.50</td><td>45.47</td><td>86.90</td><td>45.94</td></tr><tr><td>SOLAR 10.7B</td><td>11B</td><td>Pretrained</td><td>66.04</td><td>61.95</td><td>84.60</td><td>65.48</td><td>45.04</td><td>83.66</td><td>55.50</td></tr><tr><td>Qwen 14B</td><td>⇠ 14B ⇠</td><td>Pretrained</td><td>65.86</td><td>58.28</td><td>83.99</td><td>67.70</td><td>49.43</td><td>76.80</td><td>58.98</td></tr><tr><td>Mistral 7B-Instruct-v0.2</td><td>7B ⇠</td><td>Instruction-tuned</td><td>65.71</td><td>63.14</td><td>84.88</td><td>60.78</td><td>68.26</td><td>77.19</td><td>40.03</td></tr><tr><td>Yi 34B-Chat</td><td>34B ⇠</td><td>Instruction-tuned</td><td>65.32</td><td>65.44</td><td>84.16</td><td>74.90</td><td>55.37</td><td>80.11</td><td>31.92</td></tr><tr><td>Mistral 7B</td><td>7B ⇠</td><td>Pretrained</td><td>60.97</td><td>59.98</td><td>83.31</td><td>64.16</td><td>42.15</td><td>78.37</td><td>37.83</td></tr></table><p id='1' data-category='paragraph' style='font-size:16px'>Table 2: Evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with<br>other top-performing models. We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score<br>(average of six tasks). We also report the size of the models in units of billions of parameters. The type indicates the<br>training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on<br>SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold.</p><h1 id='2' style='font-size:20px'>MetaMathQA (Yu et al., 2023) dataset.</h1><br><p id='3' data-category='paragraph' style='font-size:20px'>We reformatted the instruction datasets with an<br>Alpaca-styled chat template. For datasets such as<br>OpenOrca, which are derived from FLAN (Long-<br>pre et al., 2023), we ﬁlter data that overlaps with<br>the benchmark datasets (see Tab. 8 in Appendix. C<br>for more information). The alignment datasets<br>are in the {prompt, chosen, rejected} triplet for-<br>mat. We preprocess the alignment datasets follow-<br>ing Zephyr (Tunstall et al., 2023). We use Data-<br>verse (Park et al., 2024) for data preprocessing.</p><br><p id='4' data-category='paragraph' style='font-size:20px'>Evaluation. In the HuggingFace Open LLM<br>Leaderboard (Beeching et al., 2023), six types of<br>evaluation methods are presented: ARC (Clark<br>et al., 2018), HellaSWAG (Zellers et al., 2019),<br>MMLU (Hendrycks et al., 2020), TruthfulQA (Lin<br>et al., 2022), Winogrande (Sakaguchi et al., 2021),<br>and GSM8K (Cobbe et al., 2021). We utilize these<br>datasets as benchmarks for evaluation and also re-<br>port the average scores for the six tasks, e.g., H6.<br>We either submit directly to the Open LLM Leader-<br>board or utilize Evalverse (Kim et al., 2024b) for<br>running evaluations locally.</p><br><p id='5' data-category='paragraph' style='font-size:20px'>Model merging. Model merging methods such<br>as Yadav et al. (2023) can boost model perfor-<br>mance without further training. We merge some<br>of the models that we trained in both the instruc-<br>tion and alignment tuning stages. We implement<br>our own merging methods although popular open<br>source also exist such as MergeKit3.</p><br><h1 id='6' style='font-size:20px'>4.2 Main Results</h1><br><p id='7' data-category='paragraph' style='font-size:20px'>We present evaluation results for our SOLAR<br>10.7B and SOLAR 10.7B-Instruct models along</p><br><p id='8' data-category='footnote' style='font-size:18px'>3https://github.com/cg123"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(docs[0].page_content[:5000]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(\"Splits:\", len(splits))"
      ],
      "metadata": {
        "id": "ycT0wdC18mqZ",
        "outputId": "00048640-d9d6-4252-f70f-8888259f1fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "#initiate vectorstore, retriever\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "3K112rOb8vs3"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add documnets to Chromadb\n",
        "def simplify_metadata(metadata):\n",
        "    simplified_metadata = {}\n",
        "    for key, value in metadata.items():\n",
        "        if isinstance(value, (str, int, float, bool)):\n",
        "            simplified_metadata[key] = value\n",
        "        else:\n",
        "            simplified_metadata[key] = str(value)  # Convert complex data types to string\n",
        "    return simplified_metadata"
      ],
      "metadata": {
        "id": "VILDhU5q-gSf"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in splits:\n",
        "    split.metadata = simplify_metadata(split.metadata)\n",
        "vectorstore.add_documents(splits)"
      ],
      "metadata": {
        "id": "GOEY-RU6_NNH",
        "outputId": "7f908fc5-c5b2-4bac-9b40-904236131a22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9eefa0e1-ed74-4657-909d-6bdf32089564',\n",
              " '1921d0e2-8e18-4c08-8bd3-e1b86e7850e4',\n",
              " '6ce7fa74-99bd-4918-b2f5-ac884b6c1f9f',\n",
              " '1d844f00-3e74-4487-83a0-75f5af731825',\n",
              " '9546d91c-ea16-45f5-8830-3e542f4bdc93',\n",
              " 'c402bf57-44f2-4d36-b5fc-ac6501d8abf5',\n",
              " 'd46f0c01-0e16-4918-b67e-5a0a3c27e014',\n",
              " 'f5153594-3ce3-4392-b5cc-85664f3a0e82',\n",
              " '72619ac5-4fea-496c-8977-f1015f9a9a76',\n",
              " '53d5a656-3c15-41d9-b575-ba7cc7b2af64']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "find8XkYvj_l"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "\n",
        "llm = ChatUpstage(model=\"solar-pro\")\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Please provide most correct answer from the following context.\n",
        "    Think step by step and look the html tags and table values carefully to provide the most correct answer.\n",
        "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
        "    ---\n",
        "    Question: {question}\n",
        "    ---\n",
        "    Context: {Context}\n",
        "    \"\"\"\n",
        ")\n",
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = retriever.invoke(\"Explain Table 2?\")\n",
        "\n",
        "context=\"\"\n",
        "for result in search_result:\n",
        "    context += result.page_content\n",
        "\n",
        "chain.invoke({\"question\": \"Explain Table 2?\", \"Context\": context})\n"
      ],
      "metadata": {
        "id": "K2JeqFu18k-x",
        "outputId": "77623241-6f46-4ec1-e697-858923505b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Table 2 shows the evaluation results of different language models in the Open LLM Leaderboard. The results include scores for six tasks mentioned in Section 4.1 of the source document, as well as the H6 score (average of six tasks). The table also reports the size of the models in units of billions of parameters and the type of the model, which indicates the training stage and can be chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold. The information is not present in the context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = retriever.invoke(\"what is MMLU scores of SOLAR 10.7B?\")\n",
        "\n",
        "context=\"\"\n",
        "for result in search_result:\n",
        "    context += result.page_content\n",
        "\n",
        "chain.invoke({\"question\": \"what is MMLU scores of SOLAR 10.7B?\", \"Context\": context})\n"
      ],
      "metadata": {
        "id": "ynLtUyDU06Tz",
        "outputId": "c2dcdb96-ba90-420b-d7d0-854f376f9ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The MMLU score of SOLAR 10.7B is 65.48.\\n\\n---'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "kTWmYI2-vj_m",
        "outputId": "7f3c5a97-2ae8-4aa8-b0c3-c27e1283c2af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The MMLU score of Mistral 7B-Instruct-v0.2 is 60.78.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "search_result = retriever.invoke(\"What is MMLU scores of Mistral 7B-Instruct-v0.2?\")\n",
        "\n",
        "context=\"\"\n",
        "for result in search_result:\n",
        "    context += result.page_content\n",
        "\n",
        "chain.invoke({\"question\": \"What is MMLU scores of Mistral 7B-Instruct-v0.2?\", \"Context\": context})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl1HWdZgvj_m"
      },
      "source": [
        "# Excercise\n",
        "Sometimes, even if we provide a table in Markdown or HTML format, the Large Language Model (LLM) may not extract the information correctly. How can you fix this issue?\n",
        "\n",
        "Hint: Consider using CoT, a few-shot learning approach or a divide and conquer strategy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
