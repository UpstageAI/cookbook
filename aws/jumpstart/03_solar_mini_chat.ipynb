{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Solar Mini Chat on SageMaker JumpStart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Solar Mini Chat** is an advanced English/Korean large language model developed by Upstage. Specifically fine-tuned for multi-turn chat purposes, it demonstrates enhanced performance across a wide range of natural language processing tasks. This fine-tuning equips it with the ability to handle extended conversations more effectively, making it particularly adept for interactive applications. It employs a scaling method called ‘depth up-scaling’ (DUS), which is comprised of depthwise scaling and continued pretraining. DUS allows for a much more straightforward and efficient enlargement of smaller models than other scaling methods such as mixture-of-experts. \n",
    "\n",
    "This sample notebook shows you how to deploy [Solar Mini Chat](https://aws.amazon.com/marketplace/pp/prodview-7fug6scf2nc6g) and [Solar Mini Chat - Quant](https://aws.amazon.com/marketplace/pp/prodview-npmcixwzkjoxu) using Amazon SageMaker.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Prepare input payload](#B.-Prepare-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "   1. [Delete the endpoint](#A.-Delete-the-endpoint)\n",
    "   2. [Delete the model](#B.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To subscribe to the model package:\n",
    "1. Open [Solar Mini Chat](https://aws.amazon.com/marketplace/pp/prodview-7fug6scf2nc6g) or [Solar Mini Chat - Quant](https://aws.amazon.com/marketplace/pp/prodview-npmcixwzkjoxu) model package listing page.\n",
    "2. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "4. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer two types of packages.\n",
    "* Solar-1-Mini-Chat (Support `ml.g5.12xlarge`)\n",
    "* 4-bit quantized version of Solar-1-Mini-Chat (Support `ml.g5.2xlarge`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sseclient-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import sseclient\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose one of our model packages\n",
    "# We offer two types of packages.\n",
    "\n",
    "model_package_name = \"solar-1-mini-chat-240612-r2-80ca3470b1d032c582ac66a019de67d0\"\n",
    "# model_package_name = \"solar-1-mini-chat-4bit-240612--b09baa1c640133d49d247beb85eef04d\" # 4-bit quantized model\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{model_package_name}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_package_name}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_package_name}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_package_name}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_package_name}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_package_name}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_package_name}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_package_name}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_package_name}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_package_name}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_package_name}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_package_name}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_package_name}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_package_name}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_package_name}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_package_name}\",\n",
    "}\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]\n",
    "\n",
    "print(f\"Model Package: '{model_package_arn}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"solar-1-mini-chat\"\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# ml.g5.2xlarge for quantized version, or\n",
    "# ml.g5.12xlarge for non-quantized version\n",
    "real_time_inference_instance_type = (\n",
    "    # \"ml.g5.2xlarge\"\n",
    "    \"ml.g5.12xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name)\n",
    "print(f\"endpoint name: '{endpoint_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "model.deploy(1, real_time_inference_instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Prepare input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We support request/response payload compitable to OpenAI's Chat completion endpoint.\n",
    "\n",
    "Supported parameters:\n",
    "- messages(list of objects)*: List of messages that contains `role` and `content`. `role` must be one of [`system`, `user`, `assistant`].\n",
    "- model(string): You can use `model` parameter for compitability, but since We have only one model, this parameter is not required.\n",
    "- frequency_penalty(number): Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "- presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "- max_tokens: The maximum number of tokens that can be generated in the chat completion. Solar support maximum 4k(4096) context for input and generated tokens.\n",
    "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "- top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "**required*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Single-turn chat example\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you provide a Python script to merge two sorted lists?\",\n",
    "        },\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To limit length of your output, you can use max_tokens parameter.\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you provide a Python script to merge two sorted lists?\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multi-turn chat example\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you provide a Python script to merge two sorted lists?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"Sure, here is a Python script to merge two sorted lists:\n",
    "\n",
    "                    ```python\n",
    "                    def merge_lists(list1, list2):\n",
    "                        return sorted(list1 + list2)\n",
    "                    ```\n",
    "                    \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you provide an example of how to use this function?\",\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# real-time inference\n",
    "def invoke_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    data = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = json.loads(invoke_endpoint(endpoint_name, input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"assistant: \")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. Perform real-time inference with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input[\"stream\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint_stream(endpoint_name, request_body):\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(request_body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    for event in response[\"Body\"]:\n",
    "        yield event[\"PayloadPart\"][\"Bytes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print stream response\n",
    "response = invoke_endpoint_stream(endpoint_name, input)\n",
    "\n",
    "client = sseclient.SSEClient(response)\n",
    "for event in client.events():\n",
    "    if event.data == \"[DONE]\":\n",
    "        break\n",
    "\n",
    "    data = json.loads(event.data)\n",
    "    if data.get(\"choices\"):\n",
    "        print(data[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you can delete the endpoint and avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
